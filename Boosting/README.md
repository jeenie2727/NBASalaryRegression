※final_nba_data.csv
- 선수 계약 데이터 + 계약 시작 이전 년도 5년동안의 경기 통계 연평균 데이터

- target = 계약 평균연봉 / 계약 기간동안의 샐러리캡 평균 
-> 연봉이라는 수치는 매년 조금씩 오른다. 화폐경제 시스템의 근본적인 구조에 의해,
   물가는 계속 상승할 수 밖에 없기 때문이다.
   이러한 물가 상승률을 반영하기란 매우 어렵다. 하지만, NBA의 샐러리캡 제도를 이용하여,
   이 물가 상승률을 반영할 수 있게 되었다.
   샐러리캡은 간단히 말하면, 한 팀 안에 있는 모든 선수의 연봉 합에 제한을 거는 것이다. 이는, 당연히 물가와 비슷한 흐름으로 매년
   오르므로 물가 상승률을 반영할 수 있는 지표이다. 
   따라서, 우리는 target을 연봉 / 샐러리캡으로 설정하여, 물가 상승률이란 반영하기 어려운 문제점을 해결할 수 있었다.




※NBA_Boosting.ipynb 
  : 일단 Naive하게 접근해보기로 하였다. 내 git에 올려둔 final_nba_data.csv(5년치)로
  간단하게 원 핫 인코딩, 타입을 int or float로 다 변환, 결측치 있는 행 제거만 한 이후에
  XGBOOST와 ADABOOST 모델에 별다른 튜닝없이 학습을 진행하였다.
  두 가지 CASE로 진행하였다.
  
  CASE 1: 표준화 X 모델
     xgb_test_mse: 6.852434246716843e-05
     xgb_train_mse: 3.598023670553701e-07
     xgb_test_r2: 0.9869873537351564
     xgb_train_r2: 0.9999336873363099

     ada_test_mse: 0.00024858404287796424
     ada_train_mse: 0.00018169359796815573
     ada_test_r2: 0.9527943486855712
     ada_train_r2: 0.9665133207562985
      
  
  너무 우수하게 나와서 오히려 의심스러웠다. 왜 이렇게 높게 나왔는 지 생각해보았을 때, 
  target값의 scale이 너무 작아서 그렇다고 생각하였다. target값이 엄청나게 작기 때문에, 절대적인 오차 자체가 작게 나왔고
  이가 성능을 좋게 만든 게 아닌가 생각하였다.
  하지만, 절대적인 오차에 제곱을 해 나타낸 mse도 엄청나게 작게 나온 걸로 보아, 그 추측이 맞다고는 할 수 없을 것 같다.
  일단 더 분석하기 위해, 정규화를 진행한 이후에도 진행해 보았다. 정규화를 진행해보면, 판단이 확실히 설 것 같다.


  CASE 2: 표준화 O 모델

         xgb_scaled_test_mse: 6.852434246716843e-05
         xgb_scaled_train_mse: 3.598023670553701e-07
         xgb_scaled_test_r2: 0.9869873537351564
         xgb_scaled_train_r2: 0.9999336873363099

         ada_scaled_test_mse: 0.0002111694842060017
         ada_scaled_train_mse: 0.00015676138620537
         ada_scaled_test_r2: 0.9598993043790426
         ada_scaled_train_r2: 0.9711084027375732
      

   -> 결과는 또 엄청나게 우수하게 나왔다. 정규화의 문제가 아닌 것 같다. 왜냐하면, 정규화 안한 모델에서도 
   절대적인 오차에 제곱을 한 mse 자체가 작게 나왔었고, r2은 애초에 비율로 평가하는 지표이기 때문이다.
   표준화를 하고, 안 하고의 차이는 크게 문제가 없는 것 같다.
   근데, 왜 xgb는 표준화했을 때, 안했을 때의 mse와 r2값이 같지? 뭔가 잘못된 것인가? 아님 xgb의 특성인가. 
   이걸 한 번 알아봐야겠다. 그리고 혹시 내 데이터가 잘못된 것일 수도 있으므로, 다른 데이터로 다시 시도해보아야 겠다.




※NBA_Boosting_2.ipynb
- git main directory에 있는 finaldata.csv(3년치)를 이용하여 위와 똑같은 전처리 후 
  ADABoost와 XGBoost를 돌려보았다. 혹시 내 데이터가 잘못되었을 수도 있고, 5년치와 3년치데이터를 비교해보기 위함.
  
  CASE 1: 표준화 X 모델
          xgb_test_mse: 6.893317031478567e-05
          xgb_train_mse: 3.7344312792603566e-07
          xgb_test_r2: 0.9893601110776246
          xgb_train_r2: 0.99994187460462

          ada_test_mse: 0.0002615655145532
          ada_train_mse: 0.0001776191896123923
          ada_test_r2: 0.9596271576069805
          ada_train_r2: 0.9723540618336516
  
  CASE 2: 표준화 O 모델
          xgb_scaled_test_mse: 6.893317031478567e-05
          xgb_scaled_train_mse: 3.7344312792603566e-07
          xgb_scaled_test_r2: 0.9893601110776246
          xgb_scaled_train_r2: 0.99994187460462

          ada_scaled_test_mse: 0.0002475732619793287
          ada_scaled_train_mse: 0.00015957001303206001
          ada_scaled_test_r2: 0.9617868727699415
          ada_scaled_train_r2: 0.9751633665083451

-> 또 성능이 우수하게 나온 걸로 보아, 데이터 문제가 아닌 것 같다. 그렇다면 왜 boosting이 성능이 좋게 나오는 걸까?
   그리고 왜 xgboost는 표준화 하고, 안하고의 mse와 r2값이 동일하게 나타나는 걸까?

